{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UIAqr2CPdfJe"
   },
   "source": [
    "# Урок 8. Снижение размерности данных "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oOcLKigsdfJg"
   },
   "source": [
    "Большая размерность данных (под ней понимается размерность пространства признаков, то есть их количество) может серьезно усложнить задачу анализа таких данных и даже стать причиной некорректной работы некоторых алгоритмов. Кроме того, часто в исходных данных могут присутствовать лишние признаки, никак не связанные с целевой переменной. Поэтому часто встает задача понижения количества признаков, оставляя при этом самые значимые (наиболее сильно влияющие на значение целевого параметра) с отсечением менее значимых (наиболее слабо коррелирующих со значением целевого параметра) или с формированием новых признаков на основе старых. То есть ставится задача перехода от пространства большей размерности к пространству меньшей размерности с сохранением максимального количества полезной информации."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_Pa5-7u-dfJh"
   },
   "source": [
    "## Алгоритмы снижения размерности"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zspAp10fdfJi"
   },
   "source": [
    "Алгоритмы снижения размерности пространства признаков делятся на две группы - _отбор признаков_ (то есть отбрасывание наименее важных признаков) и _понижение размерности_ путем формирования новых признаков на основе старых."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Lmvx3YA5dfJj"
   },
   "source": [
    "### Отбор признаков"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "isLulqvJdfJk"
   },
   "source": [
    "Самым простым и примитивным методом отбора является _одномерный отбор признаков_. Он заключается в оценке предсказательной силы каждого признака, то есть его информативности - насколько он коррелирует с целевой переменной. Затем отбираются либо заданное количество $k$ признаков, либо те признаки, информативность которых выше некоторого порога."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U547aIwTdfJl"
   },
   "source": [
    "Оценка предсказательной силы признака (или степени связи этого признака и целевой переменной) может проводиться разными методами, например:\n",
    "\n",
    "- в случае регрессии - _корреляция_ $$R_{j} = \\frac{\\sum_{i=1}^{l}(x_{ij} - \\bar{x}_{j})(y_{i} - \\bar{y})}{\\sqrt{\\sum_{i=1}^{l}(x_{ij} - \\bar{x}_{j})^{2}\\sum_{i=1}^{l}(y_{i} - \\bar{y})^{2}}},$$ где $\\bar{x_{j}}$ и $\\bar{y}$ - среднее значение $j$-го признака и целевой переменной, соответственно. Чем больше по модулю корреляция ($\\pm 1$), тем информативнее признак. Следует заметить, что этот метод учитывает только линейную связь между признаком и целевой переменной.\n",
    "\n",
    "\n",
    "- в случае задачи классификации - *взаимная информация (mutual information)*, моделирующая корреляцию между признаками и классами. Она использует в расчете вероятность того, что одновременно значение $j$-го признака $x_{ij}$ равно числу $v$ и значение целевой переменной $y_{i}=k$, или, другими словами, долю таких объектов от общего количества объектов в выборке $P(x=v,y=k)$. Тогда взаимная информация будет находиться как $$MI_{j}=\\sum_{v \\in X}\\sum_{k \\in Y}P(x=v,y=k)\\text{log}\\frac{P(x=v,y=k)}{P(x=v)P(y=k)}.$$ Здесь $P(x=v)$ и $P(y=k)$ - доли объектов, на которых значение признака равно $v$ и значение целевой переменной равно $k$, соответственно. Если признак и целевая переменная независимы, то взаимная информация обращается в ноль. В отличие от предыдущего метода, этот метод позволяет находять произвольные зависимости (в т.ч. нелинейные) в пространстве произвольной размерности.\n",
    "\n",
    "Такие методы позволяют оценить важность исключительно каждого признака отдельно, без учета влияния комбинаций признаков на целевую переменную, поэтому они и называются одномерными. На практике зачастую признаки влияют именно в совокупности, и по отдельности могут ошибочно быть расценены как некоррелирующие с целевой переменной, поэтому одномерные методы отбора не являются оптимальным методом в большинстве случаев."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DV5E5PnadfJl"
   },
   "source": [
    "Отдельной группой методов можно назвать так называемые *переборные методы*, которые дискретно оценивают качество модели, обученной на различных подмножествах признаков. При этом происходит полный перебор всех возможных вариантов. Обычно такие алгоритмы делятся на _жадные (greedy)_ и _нежадные (non-greedy)_. Полный список их можно найти в дополнительных материалах."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EOjQFkTgdfJm"
   },
   "source": [
    "Жадность алгоритмов заключаются в том, что если один из признаков включен в подмножество (или исключен в случае исключающего метода), в следующих итерациях поиска он уже не учитывается, так что алгоритм работает на меньшем объеме данных. Известные алгоритмы этого типа - _жадное включение_ и _жадное исключение_. В случае жадного включения на первой итерации аналогично одномерному отбору признаков находится признак, обладающий наибольшей предсказательной силой и добавляется в формирующуееся подмножество $\\{i_{1}\\}$. Далее происходит перебор оставшихся признаков с попеременным добавлением каждого из них в подмножество к первому и оценкой качества получаемой модели, обученной на подмножестве из этих двух признаков $\\{i_{1}, i_{2}\\}$. В итоге в подмножестве остается тот признак, при добавлении которого получается наилучшее качество. Далее эта процедура повторяется до момента, пока ошибка получаемой модели уменьшается. На каждой итерации в подмножество добавляется один признак, максимально улучшающий работу модели. Если на какой-то итерации при добавлении признаков ошибка не уменьшается, процесс останавливается.\n",
    "\n",
    "Плюсом такого алгоритма является относительная быстрота и возможность учета некоторых взаимодействий между признаками (как раз то, чего лишен одномерный отбор). Минусом же можно назвать вероятность застрять в локальном минимуме ошибки, если такой есть. В случае же когда есть единственный глобальный минимум, алгоритм найдет оптимальное решение.\n",
    "\n",
    "Есть также модификации этого алгоритма с многократным проходом по выборке и поочередным включением/исключением признаков из подмножества для учета совокупного влияния признаков."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q5TOTIM4dfJo"
   },
   "source": [
    "Примером нежадного алгоритма может быть простой последовательный полный перебор всех возможных подмножеств признаков. Такой подбор позволяет найти наиболее оптимальное подмножество признаков, но, очевидно, он является достаточно трудоемким (нужно перебрать $2^{n}$ вариантов, где $n$ - число признаков), поэтому подходит только для датасетов с небольшим количеством признаков."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kqDrhuvidfJp"
   },
   "source": [
    "Еще одна группа методов отбора признаков - _встроенные в модели_. Они используют эвристики, заложенные в обучающие модели, для оценки важности признаков.\n",
    "\n",
    "- Например, в случае работы с линейными моделями мы имеем зависимость целевой переменной от взвешенной суммы признаков $$a(x) = \\sum_{i=1}^{n}w_{i}x^{i}.$$ Здесь, если признаки масштабированы, веса будут являться показателями информативности признаков: чем больше вес, тем больший вклад данный признак вносит в значение целевой переменной. На основе этого показателя можно проводить отбор признаков. Также, вспоминая уроки по линейным моделям, можно упомянуть, что использование $L_{1}$-регуляризации приводит к занулению весов наименее важных признаков, то есть к их отбрасыванию, при этом больший коэффициент регуляризации будет приводить к большему количеству зануленных весов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iMFGIpqQdfJr"
   },
   "source": [
    "- В случае использования решающих деревьев и их композиций, где в каждой вершине происходит разбиение на два поддерева путем сравнивания значения одного признака с некоторым значением порога, важность признака можно оценивать по тому, насколько он уменьшает значение критерия информативности, по которому оценивается качество разбиения: $$Q(X_{m}, j, t) = H(X_{m}) - \\frac{|X_{l}|}{|X_{m}|}H(X_{l}) - \\frac{|X_{r}|}{|X_{m}|}H(X_{r}),$$ где $X_{m}$ - множество объектов, попавших в вершину на данном шаге, $X_{l}$ и $X_{r}$ - множества, попадающие в левое и правое поддерево, соответственно, после разбиения. $H(X)$ - критерий информативности. \n",
    "    \n",
    "    Чем сильнее падает критерий информативности при разбиении по данному признаку (то есть чем выше $Q$), тем этот признак важнее. Таким образом, важность $j$-го признака можно оценить путем вычисления суммы уменьшений критерия информативности по всем вершинам, в которых делалось разбиение по данному признаку. Чем больше эта сумма, тем важнее данный признак был при построении дерева. В случае композиций деревьев этот показатель суммируется по всем деревьям."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "suyYET13dfJr"
   },
   "source": [
    "### Понижение размерности"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8w8aSllUdfJs"
   },
   "source": [
    "Кроме отбора признаков, который не всегда оптимален в плане сохранения максимума полезной информации, существуют еще методы понижения размерности путем формирования новых признаков на основе старых. Новых признаков при использовании такого метода должно быть меньше, чем исходных, при условии сохранения максимально возможного количества информации из исходных признаков. Например, объединение нескольких признаков в линейную комбинацию:\n",
    "\n",
    "$$z_{ij}=\\sum_{k=1}^{n}w_{jk}x_{ik},$$\n",
    "\n",
    "где $x_{ij}$ - исходные признаки, $z_{ij}$ - новые принаки."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U2iJ7XBhdfJt"
   },
   "source": [
    "Простейшим методов такого понижения размерности является метод *случайных проекций*, который заключается в преобразованиях, сохраняющих расстояния и снижающих размерности. Существование таких преобразований доказано для выборок, в которых объектов меньше, чем признаков. Веса при всех признаках в таком методе можно выбирать случайно. При этом не факт, что мы попадем в оптимальное преобразование, но практика показывает, что метод работает, если размерность нового пространства признаков\n",
    "\n",
    "$$d > \\frac{8\\text{ln}l}{\\varepsilon^{2}},$$\n",
    "\n",
    "где $l$ - количество объектов, $\\varepsilon$ - максимальное изменение расстояния между объектами (лемма о малом искажении или лемма Джонсона-Линденштрауса)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7RKYQxgMdfJu"
   },
   "source": [
    "### Метод главных компонент (PCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A7frA6OrdfJv"
   },
   "source": [
    "Одним из наиболее известных и широко применяемых методов понижения размерности является _метод главных компонент (principal component analysis, PCA)_. Он заключается в приближении матрицы признаков матрицей меньшего ранга - так называемом низкоранговом приближении.\n",
    "\n",
    "Запишем показанную ранее формулу линейного преобразования признаков в матричном виде:\n",
    "\n",
    "$$Z = XW^{T},$$\n",
    "\n",
    "где $X$ - матрица \"объекты-признаки\", где по строкам отложены объекты, а по столбцам - значения признаков, $Z$ - матрица новых признаков, $W^{T}$ - транспонированная матрица весов. Приближение заключается формировании новой матрицы признаков $\\tilde{X}=ZW\\approx X$ с возможностью восстанавливания старых признаков по новым с максимальным уровнем точности, или, если говорить иначе, чтобы их различие было минимальным:\n",
    "\n",
    "$$\\|ZW - X\\|^{2} \\rightarrow \\underset{Z, W}{\\text{min}}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6oGhpkBfdfJv"
   },
   "source": [
    "При этом метод главных компонент предполагает, что матрица весов должна быть ортогональной, то есть произведение $WW^{T}$ должно равняться единичной матрице. Восстановленная матрица $ZW$ может иметь ранг меньший, чем исходная $X$, поэтому приближение будет называться низкоранговым."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P45TTDjedfJw"
   },
   "source": [
    "Геометрически метод можно представить как проецирование признаков на гиперплоскость с максимизацией дисперсии получаемой выборки."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3dV8i8cDdfJx"
   },
   "source": [
    "Если ранг матрицы исходных признаков $rank(X) \\geq d$, где $d$ - число новых признаков, то минимум функционала различия, описанного выше, достигается тогда, когда в качестве строк матрицы $W$ используются собственные векторы матрицы $X^{T}X$ , соответствующие максимальным собственным значениям $\\lambda_{1},...,\\lambda_{d}$. Максимальные собственные значения и называются __главными компонентами__, от чего пошло название метода. Первая главная компонента соответствует максимальному собственному значению и т.д."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VvLN4XJLdfJx"
   },
   "source": [
    "Некоторые полезные свойства метода:\n",
    "\n",
    "- Матрица $Z$ при этом будет такой, что $Z^{T}Z = \\Lambda = diag(\\lambda_{1},...,\\lambda_{d})$.\n",
    "\n",
    "\n",
    "- Минимизированный функционал ошибки будет равен $$\\|ZW - X\\|^{2} = \\|X\\|^{2} - tr\\Lambda,$$ где $tr\\Lambda,$ - след матрицы $\\Lambda$, то есть сумма всех собственных значений $\\lambda_{1},...,\\lambda_{d}$, а $\\|X\\|^{2}$ - сумма всех собственных значений исходной матрицы $\\lambda_{1},...,\\lambda_{n}$, таким образом $$\\|ZW - X\\|^{2} = \\sum_{j=d+1}^{n}\\lambda_{j},$$ то есть значение функционала ошибки будет равно сумме собственных значений, которые не были взяты в получаемое разложение. Поэтому логично брать в разложение максимальные собственные значения, оставляя минимальные.\n",
    "\n",
    "\n",
    "- Матрица $X^{T}X$ - матрица ковариации, то есть матрица, которая характеризует дисперсию выборки. Дисперсия выборки после проецирования будет равна собственному значению $\\lambda$, поэтому логично, что первым берется собственный вектор, соответствующий максимальному собственному значению - нам нужно сохранить максимум дисперсии."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-pPzjbW8dfJy"
   },
   "source": [
    "Таким образом, для реализации метода главных компонент нужно :\n",
    "- найти собственные значения матрицы $X^{T}X$;\n",
    "- отобрать $d$ максимальных;\n",
    "- составить матрицу $W^{T}$, столбцы которой будут являться собственными векторами, соответствующими отобранным собственным значениям, расположенным в порядке убывания;\n",
    "- получить новую матрицу \"объекты-признаки\", умножив исходную матрицу $X$ на матрицу весов $W$:\n",
    "\n",
    "$$Z=XW.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LnYvDiOadfJz"
   },
   "source": [
    "### Сингулярное разложение (SVD) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Thkg-tSXdfJ0"
   },
   "source": [
    "Сформулировав принцип реализации метода главных компонент, нельзя не заметить его родство с _сингулярным разложением матриц (singular value decomposition, SVD)_. Вспомним, что сингулярное разложение матрицы - это разложение вида\n",
    "\n",
    "$$X=UDV^{T},$$\n",
    "\n",
    "где столбцы ортогональной матрицы $U$ - это собственные векторы матрицы $XX^{T}$, столбцы ортогональной матрицы $V$ - собственные векторы матрицы $X^{T}X$, а на главной диагонали диагональной матрицы $D$ расположены собственные значения матриц $XX^{T}$ и $X^{T}X$ (они равны и также называются сингулярными числами матрицы $X$).\n",
    "\n",
    "Если число новых признаков $d$ равно старому числу признаков $n$, то можно приравнять разложения\n",
    "\n",
    "$$X=ZW=UDV^{T}.$$\n",
    "\n",
    "При этом матрицы $W$ и $V^{T}$ состоят из собственных векторов матрицы $X^{T}X$, то есть они равны при $Z=UD$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8tzfpn9zdfJ0"
   },
   "source": [
    "Получается, что метод главных компонент - в своем роде \"урезанная версия\" сингулярного разложения, из которого убрали минимальные собственные значения с соответствующими собственными векторами. \n",
    "Таким образом, для реализации понижения размерности методом главные компонент с помощью SVD нужно:\n",
    "- найти сингулярное разложение вектора $X$;\n",
    "- сформировать из столбцов матрицы $V$, соответствующих $d$ наибольшим сингулярным числам, матрицу весов $W$;\n",
    "- получить новую матрицу \"объекты-признаки\", умножив исходную матрицу $X$ на матрицу весов $W$:\n",
    "\n",
    "$$Z=XW.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dWEQalcAdfJ1"
   },
   "source": [
    "Для закрепления теории реализуем PCA с помощью Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kSmUQy4ZdfJ2"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m3d93Gx2dfJ6",
    "outputId": "eaee1c83-f56e-474c-a5f2-bdd5d4659ab4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 4)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Загрузим игрушечный датасет из sklearn\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eE6X-RyIdfJ-"
   },
   "outputs": [],
   "source": [
    "# Для начала отмасштабируем выборку\n",
    "X_ = X.astype(float)\n",
    "\n",
    "rows, cols = X_.shape\n",
    "\n",
    "# центрирование - вычитание из каждого значения среднего по строке\n",
    "means = X_.mean(0)\n",
    "for i in range(rows):\n",
    "    for j in range(cols):\n",
    "        X_[i, j] -= means[j]\n",
    "\n",
    "# деление каждого значения на стандартное отклонение\n",
    "std = np.std(X_, axis=0)\n",
    "for i in range(cols):\n",
    "    for j in range(rows):\n",
    "        X_[j][i] /= std[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ItFOmLW9dfKB",
    "outputId": "0b507805-c106-4539-d15a-4dc8327b764f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Собственные значения в порядке убывания:\n",
      "(437.7746724797989, array([ 0.52106591, -0.26934744,  0.5804131 ,  0.56485654]))\n",
      "(137.1045707202105, array([-0.37741762, -0.92329566, -0.02449161, -0.06694199]))\n",
      "(22.013531335697213, array([-0.71956635,  0.24438178,  0.14212637,  0.63427274]))\n",
      "(3.107225464292848, array([ 0.26128628, -0.12350962, -0.80144925,  0.52359713]))\n"
     ]
    }
   ],
   "source": [
    "# Найдем собственные векторы и собственные значения (англ. Eigenvalues)\n",
    " \n",
    "covariance_matrix = X_.T.dot(X_)\n",
    "\n",
    "eig_values, eig_vectors = np.linalg.eig(covariance_matrix)\n",
    "\n",
    "# сформируем список кортежей (собственное значение, собственный вектор)\n",
    "eig_pairs = [(np.abs(eig_values[i]), eig_vectors[:, i]) for i in range(len(eig_values))]\n",
    "\n",
    "# и отсортируем список по убыванию собственных значений\n",
    "eig_pairs.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "print('Собственные значения в порядке убывания:')\n",
    "for i in eig_pairs:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9mdEDwm2dfKD"
   },
   "source": [
    "Оценим долю дисперсии, которая описывается найденными компонентами."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ct-1I70mdfKE",
    "outputId": "ab04c5c2-649d-4f6f-e849-3791203481d2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Доля дисперсии, описываемая каждой из компонент \n",
      "[72.96244541329989, 22.850761786701774, 3.668921889282873, 0.5178709107154752]\n",
      "Кумулятивная доля дисперсии по компонентам \n",
      "[ 72.96244541  95.8132072   99.48212909 100.        ]\n"
     ]
    }
   ],
   "source": [
    "eig_sum = sum(eig_values)\n",
    "var_exp = [(i / eig_sum) * 100 for i in sorted(eig_values, reverse=True)]\n",
    "cum_var_exp = np.cumsum(var_exp)\n",
    "print(f'Доля дисперсии, описываемая каждой из компонент \\n{var_exp}')\n",
    "\n",
    "# а теперя оценим кумулятивную (то есть накапливаемую) дисперсию при учитывании каждой из компонент\n",
    "print(f'Кумулятивная доля дисперсии по компонентам \\n{cum_var_exp}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NE15sfLtdfKH"
   },
   "source": [
    "Таким образом, первая главная компонента описывает почти 73% информации, а первые две в сумме - 95.8%. В то же время последняя компонента описывает всего 0.5% и может быть отброжена без страха значительных потерь в качестве нашего анализа. Мы отбросим последние две компоненты, оставив первые две."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z4xcGvf7dfKI",
    "outputId": "e406100f-3bd4-40fb-ce09-15db12e77a12"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Матрица весов W:\n",
      " [[ 0.52106591 -0.37741762]\n",
      " [-0.26934744 -0.92329566]\n",
      " [ 0.5804131  -0.02449161]\n",
      " [ 0.56485654 -0.06694199]]\n"
     ]
    }
   ],
   "source": [
    "# Сформируем вектор весов из собственных векторов, соответствующих первым двум главным компонентам\n",
    "W = np.hstack((eig_pairs[0][1].reshape(4,1), eig_pairs[1][1].reshape(4,1)))\n",
    "\n",
    "print(f'Матрица весов W:\\n', W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n04tyd6xdfKL"
   },
   "outputs": [],
   "source": [
    "# Сформируем новую матрицу \"объекты-признаки\"\n",
    "Z = X_.dot(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.26470281, -0.4800266 ],\n",
       "       [-2.08096115,  0.67413356],\n",
       "       [-2.36422905,  0.34190802],\n",
       "       [-2.29938422,  0.59739451],\n",
       "       [-2.38984217, -0.64683538],\n",
       "       [-2.07563095, -1.48917752],\n",
       "       [-2.44402884, -0.0476442 ],\n",
       "       [-2.23284716, -0.22314807],\n",
       "       [-2.33464048,  1.11532768],\n",
       "       [-2.18432817,  0.46901356]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components = 2)\n",
    "XPCAreduced = pca.fit_transform(X_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.26470281,  0.4800266 ],\n",
       "       [-2.08096115, -0.67413356],\n",
       "       [-2.36422905, -0.34190802],\n",
       "       [-2.29938422, -0.59739451],\n",
       "       [-2.38984217,  0.64683538],\n",
       "       [-2.07563095,  1.48917752],\n",
       "       [-2.44402884,  0.0476442 ],\n",
       "       [-2.23284716,  0.22314807],\n",
       "       [-2.33464048, -1.11532768],\n",
       "       [-2.18432817, -0.46901356]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XPCAreduced[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CBqTYO6udfKO",
    "outputId": "8e8e6a9d-f8fa-4c1a-ea3f-84a9bd859bed"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAEWCAYAAABv+EDhAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nO3dfZwdZX338c8vySJZ0IVs8qqK7C4Vbm01oCXYelufQKtFAbVa5V4wQDUSqoJPWElvCbVpbX0AbrwpRoum7grWig9BFBEFtFVLQEwQfEDZBXwMiQQhwTzsr3/MnOTs2Zkzcx7mzMyZ7/v1mtfumTNn5jq758w1c12/63eZuyMiItUzL+8CiIhIPlQBiIhUlCoAEZGKUgUgIlJRqgBERCpKFYCISEWpAhARqShVACIiFaUKQHrGzKbMbIeZPWRmvzKzj5vZgXXPv8jMbjKz35rZZjO70cxObNjH88zMzeydvX8H5WBmq81sou6xm9nD4d/9Z2b2QTObX/f8DWb2urrH55nZ3eH295nZp3r9HqQ3VAFIr53g7gcCfwQsA/4WwMxeCXwa+DfgCcDvAe8GTmh4/XJgK/DaXhW4TxwV/t2fC7waOCNqIzNbDpwKvCDcfhlwfc9KKT2lCkBy4e4/A74EPNXMDPgg8B53/6i7b3P3GXe/0d1fX3uNmR0AvBL4a+AIM1vW7BhmtqDh6neXmf193fMXm9m9Zvagmd1iZs8O1z8z3L72mp11j0fM7Ilm9jUz22Jm95vZpJkdVLffQ83sqvAuZouZfajuuTPM7E4z+42ZXWtmo+H69eH+Hw7LXDveZeHzf2NmPwnvju4ws5e3+Xe/C/hP4GkxmxwDXOvuPwm3/6W7r23nWFJ8qgAkF2Z2KHA88F3gScChwH8kvOwVwEMEdwrXEtwNND1M+PMPw6vZyYbnbyY4ES4CPgl82sz2d/dvufuBda/559pjd78n3O8/Ao8H/iAs++rwfc0HrgamgTHgEODK8LmTgPPC97EE+AZwBYC71+6MnhKW7aDweGeGj38CPBsYAi4AJszscQnvf+4fxOzJ4X7uitnk28BrzewdZrasvqlI+o8qAOm1z5nZA8A3gRuBfwCGw+d+kfDa5cCn3H0PwQn7NWY20GT7heHPnVFPuvuEu29x993u/gHgUQSVUVPufpe7X+fuv3P3zQR3L88Nn34GQcXwDnd/2N0fcfdvhs+dCfyju9/p7rsJ3vvTancBCcf8tLv/PLwz+hTw4/BYad1qZg8DdwI3AJfGHGcCeBPwIoL/z6/V39K/VAFIr73M3Q9y91F3P8vddwBbwudir2jDO4bns+8q/vPA/sBLmhzrscBM3f4b9/n2sDlmW1gpDQGLk96Amf2emV0Zdqg+CEzUve5QYDo8wTcaBS42swfC420luJs4JMUxX2tmt9W99qlpylrnj4ADCdr//xg4IG5Dd5909xcABxFUWu8xsxe1cCwpCVUAUgQ/BO4F/qLJNqcSfF7Xm9kvgZ8SVADNmoGeDvzA3efcAYTt/ecCfwkc7O4HAdvY12zUzD8ADix198cAp9S97l5gxMwWRLzuXuANYQVYWxa6+381O1h4h/AR4I3AcFjW21OWdS8P/DvwLYIO9qTtd7n7p4GNBBWO9BlVAJI7DyaleCvwf83sdDN7jJnNM7M/NbNaB+Rygrbvp9UtfwEcb2bDjfs0s/0Irl6viDnso4HdwGZggZm9G3hMyiI/mqAvYpuZHQK8o+65/yZoynqvmR1gZvub2bPC5y4D3mVmTwnLOGRmr0pxvAMIKpzN4etOp7MT8nuB15vZYxufMLPTzOwlZvbo8H/w5wT9Et/p4HhSUKoApBDc/T/YF574c+BXwN8DnzezPyFoPvn/YVRKbfkCQWfmyRG7vBp4HnBeLaIGGAfODa/+rwW+DPyIoMP2EYIr9DQuIGhS2QZ8Ebiq7n3sIQhdPRy4B7gvfF+4+2eBfwKuDJuObgf+PMXf5g7gAwRX7r8ClhJE8rTF3TcBNzG74qp5kKCj+h7gAeCfgZV1/RjSR0wzgkk/MrMbgNPcfaph/d8C33T3G3Iolkih6A5A+tVmgiaeRg8Cv+txWUQKSXcAIiIVpTsAEZGKigpVK6zFixf72NhY3sUQESmVW2655X53X9K4vlQVwNjYGBs2bMi7GCIipWJm01Hr1QQkIlJRqgBERCpKFYCISEWpAhARqShVACIiFaUKQCpnctMkYxeNMe+CeYxdNMbkpsZ5YkSqoVRhoCKdmtw0yYr1K9i+azsA09umWbF+BQDjS8fzLJpIz+kOQCpl1fWr9p78a7bv2s6q61flVCKR/KgCkEq5Z9s9La0X6WeqAKRSRoZGWlov0s9UAUilrDluDYMDg7PWDQ4Msua4NTmVSCQ/qgCkUsaXjrP2hLWMDo1iGKNDo6w9Ya06gKWSSjUfwLJly1zJ4EREWmNmt7j7ssb1ugMQEakoVQAiIhWlCkBEpKJUAYiIVJQqgKKanISxMZg3L/g5qXw1ItJdygVURJOTsGIFbA9TFkxPB48BxhWuKCLdoTuAIlq1at/Jv2b79mC9iEiXqAIoonti8tLErRcRaYMqgCIaiclLE7deRKQNqgCKaM0aGJydr4bBwWC9iEiXqAIoovFxWLsWRkfBLPi5dq06gEWkqxQFVFTj4zrhi0imdAcgIlJRqgBERCpKFYCISEWpAhARqajcKgAzO9TMvm5md5jZ983s7LzKIuU2uWmSsYvGmHfBPMYuGmNyk/ImiaSRZxTQbuBt7n6rmT0auMXMrnP3O3Isk5TM5KZJVqxfwfZdQeqM6W3TrFgf5E3SNI8izeV2B+Duv3D3W8PffwvcCRySV3mknFZdv2rvyb9m+67trLpeeZNEkhSiD8DMxoCnA9+JeG6FmW0wsw2bN2/uddGk4O7ZFp0fKW69iOyTewVgZgcCnwHOcfcHG59397Xuvszdly1ZsqT3BZRCGxmKzo8Ut15E9sm1AjCzAYKT/6S7X5VnWaSc1hy3hsGB2XmTBgcGWXOc8iaJJMkzCsiAfwXudPcP5lUOKY+oaJ/xpeOsPWEto0OjGMbo0ChrT1irDmCRFPKMAnoWcCqwycxuC9ed5+7X5FgmKaikaB+d8EVaZ+6edxlSW7ZsmW/YsCHvYkgOxi4aY3rb9Jz1o0OjTJ0z1fsCiZSImd3i7ssa1+feCSySRj9G+2gAm+RNFYCUQhmjfZqd4GtNWtPbpnF8b5OWKgHpJVUAUgpli/ZJOsEXbQDb5CSMjcG8ecHPSdVDlaAKQEqhbNE+SSf4IjVpTU7CihUwPQ3uwc8VK1QJVIE6gUUyMO+CeThzv1uGMXP+TKE6tcfGgpP+nLKMwlRviyIZUSewSA8l9VkUpUlrctMk09Mzkc/dU97+dUlJFYBIBpJO8EVo0qr1UzAUfaYfKW7/unSJKgCROt0KzUxzgh9fOs7UOVPMnD/D1DlTPe/P2NtPcdx5MPDwrOcGB2FNMfvXpYtUAUjpZBU/Hxe5c9YXz2rpeLWImlOPGoeLpvjE4fmc4JPs7XA+8go44fUwNAXMwNAUa9fCeLGKKxlQJ7CUSmNKCAiaVrrRfBLXMWvYrA7dZserRdRsrwsAGhykkCfUInVES7bUCSx9Icv4+bgQzMZonmbHW7Vq9skfgserCjg/TVE6oiU/qgCkVLKMn29lVHFsOWKKUcSImiJ0REu+YiuAcNL2K83sG2Z2Xpi7v/bc53pTPJHZskwJEXVFbFhr5YgpRlEjavLuiJZ8NbsDuBy4AXgT8DjgRjMbDp8bzbhcIpGybLaIuiI+c9mZLR1vzZqgzX/W9oqokaJy98gFuK3h8SnA94EnArfGvS7L5eijj3aRiY0TPnrhqNtq89ELR31i40Shjjcx4T466m4W/JzItnjx5ejx30mKC9jgEefU2CggM/s+cLS7P1K37gXAZcAB7v64bKumuRQFJJJOltFSUj7tRAF9FPjj+hXu/lXgVcDt3S2eSHkVMa9/0bKNSjHFTgnp7hfGrP8u8MLMSiRSUJObJll1/Sru2XYPI0Mje/sBmk1VmZciZRuV4spzTmCR0oibk3jhgoWxV9p5VgAjQyORg7yKPIGO9J7GAYikENeksmXHlsjt877S1iAvSUMVgEgKrZ7Q877S1iAvSSNVE5CZvQR4CrB/bZ27/11WhRIpmrgmleGFw+zYvWNOtE0RrrTHl47rhC9NJd4BmNllwKsJBoQZQRSQBoJJpcQ1qVz85xf3xZW25gSupjR3AP/b3Y80s43ufoGZfQD4UtYFEymS2gm9MQqotr5sJ/x6jRlMa3MCQ/EymEp3pekD2BH+3G5mjwd2EaSGqDZdMlVOXN6cIo4DaEWZMphKd6W5A7jazA4C3gfcCjjwkUxLVXS6ZJJQXHgolOeuoEwZTKW7Eu8A3P097v6Au3+GoO3/ye7+7uyLVmC6ZOornVzB98OI27JlMJXuSdMJfGvtd3f/nbtvy7ZIJaBLpr4RNQ3kqVedil1gqSqDfhhxqwym1ZWmDyA6IXoXmNnlZvZrMytXbiFdMvWNqCv42gxgteacZpVAlvMT9Mr4eDBl5egomAU/iziFpXRfmgrgSWa2sW7ZZGYbu3T8jwMv7tK+ekeXTH0j6Uo9qTknbhKZ6W3TpeoQHh+HqSmYmQl+6uRfDWkqgLuBE+qWl4Y/O+buNwFbu7GvtrQbyaNLpr6R5kq9WSVRP+IWZk8gn+YOQiRPaSqAne4+3bhkXrKQma0wsw1mtmHz5s3d23Etkmd6Gtz3RfK0Ugnokqn0oq7gGyVVErXw0NGh0ZYmkC+KsoexSvvSVABvyrwUTbj7Wndf5u7LlixZ0r0dtxLJo5j/vhV1BV+vlbQOZewQjuoEb/WuRV+P8oqdEWzvBmbnA3M26lYuIDMbA65296cmbdvVGcHmzQuu/OcWKLiqr2mM+YegvV9NPn0pKud/2nj+sYvGIvMFjQ6NMnXOVJdL2h2dlllfj3KImxEsTQXwtvDXc4CLauvd/QNdKtgYeVQAY2NBs0+j0dGgSafV7aSyapXG9LbpWX0AUPxpGOddMG9OsxUEd0Iz589EvGI2fT3KoZ0pIYHgRB+e7O+v/d7Fk/8VwLcIIo3uM7O/6sZ+U0kbyaOYf2mivgkFghDSWjNS0RPDTU7CvIvvgdV74MK7YePJe59LG8aqr0e5tTIjWPNbhTa4+8nJW2Wkdn+6alXwaR0ZCU7+jfetIyPRlziK+a+cqOahuHEERW72gX1NN3u2PyFYsW0M1gcZXgaP/nzqfg99PcotTRPQeoKT/3OAm2rr3f3EbIs2V1ebgNJSI6cwN+cPwMC8AXbN7IrcPm0TSl7imm7mH3wf6268MfVdi74e5RDXBJTmDuD94c+uNPuUTto7BelrUVf6cSd/KP5I4LgmmpkHntBSk5W+HuWWpg/gRmAKGAh/v5kgK2h1tBLz3ywmTvFypdVKKGdRZgRrppvZTLIaEqOvS/YS7wDM7PXACmAR8ETgEOAy4Lhsi1ZCzdJEg1JIl1jclJBRitzxC3DWWXDvvXPXFymbiTKu90aaPoDbgGcA33H3p4frNrn70h6Ub5Zc+gBa0SwmDhQvV2JRfQBRit75e9ZZ8C//Mnf9AQfAhz9cnJOrwku7q+0wUOB37r6zbkcLyCAiqC80i4mL+jRDsF73uoVXP2LYMIYXDrPf/P1mbVPEpp/GNA8f/nB0x/QjjxTn5A8KL+2VNBXAjWZ2HrDQzF4IfBpYn22xSqT+5D0v5s85MgLz50c/N29eZzmJpGfqp4S8/9z7ufykyxleOLz3+YULFuZYurmi0jzMzERnd9+zp8eFS6CM672RpgL4G2AzsAl4A3AN8LdZFqo0GhPKRX2Lag2rcd+wmRnNLlZiO3bv2Pv7lh1b9ubRySvBWv31yPLnPpftt5w0ewOL/hzGXZ/kRRnXeyOxD6BICtcHEBtMPT84sdfHxMVtG6cxJ5EUTlweneGFw+zYvWNWf0EvUkJExeQz8DCc8Ho48org8dWXwIa/pnGep5Ur4dJLMytaWyYnFV7aLZ3kArqb2W3+Bri7/353i5iscBVA2oRyED9iZuFC2LJl7j7U21V4cXl04mTdQRx7jTE0BW85bO/DA77ycR75znL27AmuVVasKN7JX7qrk07gZQRRQA8Bx4SPj+lu8UoqTUNl7Z781FODk/3w8OxJZC6+WPe6JdXqYK+s00LHdpBu21fOwYFBPnzZAnbvDq5ddu/Wyb/K0gwE2+Lu9wO7w9+3uHvEJWsfaDUap1lD5eQkLF4Mp5yyr49gyxbYuhXOPHPfiBnNLlZaUZPJDA4MzuoYrpf16OC465H5B/8cw1pKTqfAtIpw96YLwQCwRcD3gINrj5Nel8Vy9NFHe2YmJtwHB92DU3WwDA4G65NeNzrqbhb8nJiI3lf9Ypa8X+mZiY0TPnrhqNtq89ELR31iY/r/TdRrJzZO+OCaQWc1e5fBNYMt7bcd7X6EW9lP1Mddig/Y4FHn96iVszYI5gT+afiztvw06XVZLJlWAKOj0Sfr0dHu7avT/UrXZXWyjqsYmlU0nVREe/fRhRN03Md3eLh5BaPKobjiKgBFAdWk6dBNG5YQt6+4/Upu2p0Rq9WZw6JGEtdHBiU930tpPr71RkeDr4KyghZXJ1FAA8BKgnTQADcAH3b3+FSIGcm0Akgae95K3ts0IZ+K8imEdmbEaudknVTRFGk6yXYiluPmBdDHvBg6iQL6F+Bo4NJwOTpc11+SRp7ETSK/fHlQOdT3mj30EAwMxB9LUT6FEdcx26zDNio19PZd21l1ffzgvaQJ44s0oXzcV2E4um+bkRGlbiirNBXAMe6+3N2/Fi6n049hoEnROHGf5D174PTT4YwzZkf7mO0L+Rwenhv+qfviQoiL5GmW06edk3VSRdNORdSJZlE+cV+FZhHLSt1QTmkqgD1m9sTaAzP7faBgmUO6pFli82af5F27YOfO2et27oQDDwz2df/9wdLthOnSscYkb2lCJds5WSdVNO1URK2qnfTNgmEpzdJPRX0Vml0jKXVDSUX1DNcvBHn/7yFo+69NDvP8pNdlsWQaBZQkKbQzLtxT+s7Kq1fOihqqLSuvXtn0db2IAoo9doqPb6eBaYoCKi46iQIys0cBTwof/tDdf5dBXZQo91QQk5NBm3/a1InqASutZlE+WXbYthpdlFaajl0FpvWvtjuBzey1wKuBp4XLq8N11TM+DuvWzb3XHRiA/Wbnhscs+MZpGGXpRKVRrmX5hOZ9AJ1kAU06bifvZ3o6+cyu9vrqSdMH8H725f+pLXNqksqIagj92Mfg8sv3zfxlti+QWvn9Sycuymf5Z5czuWkytq1/0cJFHZ3A4457ylWntJ1SulapMNQ8HKdX7fWNnc9nnaWUE3lKMw7gux5OBZm33JuA0tBcdqXXLMvn4MAgy49azrrvrZszDmDhgoVs2TE3TVazpqH6Jp+kzKLtDAzb21y18WRY/xHYdcDe52rXKbWBXFnHJkSmq26gwWPZ6GQcQHmGCheBAqJLr1k0z/Zd27nmx9dERg5t3bE18jVxTUaNTT5JksYaND32kVcE8wIMTQEzMDTFJz4RVAC9CkyLGkrTSHMh9VaaO4BfA1c2rnf3N2dVqDi6A5BeSJoAPm6UcKudw3HbN9NshHKUIo0wTptiQp3R3dfJHcA7gFsiFomigOjSq40NmG/R8yTG3SHExfIff8TxkR3D7YzybXVgWNrxBb2YwjJtJ7M6o3snzXwA66KWXhSulJTfvy+MLx1n3cvXtTQ4K2pQWa2/IKpjOO5kPrxwmP3m7zdn/cC8gZYHhqUZ6JY2+qjTOQKiro0a6Vqpt9I0Af20cRVdmhLSzF4MXAzMBz7q7u9ttn0pmoCkr3Qal9+sCWbNcWsik8rFdSYPLxzm/nPvjy9rm3PopmkmaiUXYjONZTz+eLjmGs37m7VOsoFeCTwW+CSwHtgJwUxhHRZoPvAj4IXAfcDNwMnufkfca0pTAWg2awklZRutVTDT26aZb/PZ4/GDDOPa/ycn4eyz504tnfYEnSYjqrq28tON00nbfQDu/hrg5cAA8DHgjZ2e/EPPAO5y95+6+06CjuaTurDf7mvl3rd2qdQs0YpURlLeoPGl43vb6Zud/OP2NTkJZ7xu95yTP6SPqEmT20jBbfnI+nSSphMYYIbuh4MeAtxb9/i+cN0sZrbCzDaY2YbNmzd3uQgptPofiEsbrdi2SkrTCRs1AKxRXN/D2e94iJ2PLIh9XZrB6GnKqGyf+cj6dJImFcQngc8Bu4HTgEvMbFF3Dp/M3de6+zJ3X7ZkyZJeHXafVv8D3b5U0uzcpZamE7ZZNFBShtItv0joVSX5miVNGRXclo/M77yiMsTVLwTZP/fOBUyX5gQGnglcW/f4XcC7mr0ml2ygZvGZPqPSH3ZzbuFuzfIthTZ64WhkdtHRC0cTX8vQ3amT0yrbZ/l063RCTDbQNH0AY+5+WLj8fu1nF+qem4EjzOwwM9sPeA3whS7st7vi7nEXLYpuGjr88CD8s167l0pqTqqETuYCGH7pB2Hg4Ya10a21nV41NpsuQ7KR9Z1XmiagATN7s5n9R7i8MZwnuCPuvht4I3AtcCfw7+7+/U7323Vx/wGIPjl/7WuzhzuaBSmk2/m2qOetEtqZlKbm4nf+MQMve+OsFA8sjI7RUHt9+WQ9rChNGOhHCSKAaoO/TgX2uPvrulOE9HILA42Kwzr11HTj2qH9WDnF3kkKjWMVjn9kgnXv+dOOY/alf8SFgaZpq/9emnW9WHKdEaxRXONc0sxgrTSkqg9A2qT2eqlHu30AVGlO4FakGddeM29ecBfRakip0kpIE83y92TZXl8/t/CCBcHPMgWoKbCuTlStUL8QPSfwsUmvy2Ip1B2A++zLrPnzm98FDA66Dw9nE54hlTOxccIH1wzOihoaXDPY1XmEI4/bZG7hdm5Oe32nUtWbajQncMbS5rqNovy30qK80jwnzS3cSvdUt/ILtaKq3WqdzAn8Anf/nbtvdPeNwGPC/EBSr5MQC4VnSIuazUvcLVFNTEkBaK0EqOUR5azAutnS9AGsNrPXAJjZ6cBNBCODpV4rfQL1NJxS2pAmf08n4lJEL3rsQ83L1cLh8zgZK6XFbGkqgBcDrzWzW4HnAM9yd90BNKrvsIW5g8EaqVNXUojr6O1k8FgacRPUc+x5sdc5rV7L5HEyVkqL2dJUAPsBZwA/A7YC3stcQKVSC71wh098AuZHzyjF6Gh0eEZjeMJZZylcocKaTdTSyeCxNKa/8Sy48G5YvSf4ufFkALYe8aFZ1zm1j3g71zLHHx+9/vDDOyh4KC7SJ+vAutJFGEX1DNcvzM4BtDcnUNLrsljaigLqRZhB3DFaCTloFl5RpXAF2auTHEGdmJhwt4GHZ3/8Bh5yXnFyV48dN5SmlmarXXlF+hQ5woiYKKCen8Q7WVquAHrxH4k6hpn7ypX7nk+qgCYmksNIFTJaObbaIisAW23JL+5A7In5oOmuhpnG5Vns9GPezXyMZThuGnEVQKow0KJoOQy0FzFfaeLimk3hExUL14xCRisjKdSz0+kq48RGNJvjMwl9Wy1o9tXp5GMeV/6svzp5HTeNtsNAS60XYQZJ+5qeDvIGnXVW9PNRsXDNVDVcoYKadfSmmci92UjhZuI+YqMj3Tv5Q3BdFBcrkUVUddZfnTJGGPV3BdCL/0iafbnDZZdF9wi1UhlVOVyhgpp19MZF6ay6PgiiT1NBxOlVpMz4OBx77Nz1rR6rseP1+OPzifQpY4RRmoFgz4laelG4jrX7H2mlK7/ZZUw99+gRLnEVyPz5sHKl8gBJpKSBYEkVRDO9SkE1OQnf+tbsda1mT49Kr7VuXbCPXn91Spm6K6pjoH4BHiCYqGU98Jvw5xeSXpfF0pMooHY6jleubN6jFZUVtJPjSSU0y/eTFCHUSgdybX+22nz0wtHM8wnVpOk0Tfr6FrnjtUjoIBvo3e5+orufQDBx+4nufmJG9VH3tZoWsdn49Lg7g0svDeL+a8HRcaKu9kt52SC90OwqPmkgWNqRwp00FXUqqYsuTfJcpXboTJoKYH8zGzazw4AlwJfMLIfZ2Xsk7pNT+/TFfRrrB4GtXNl8WsjGigQ0157M0ayZJ2kgWNqRwp00FXUqqYsuTa6gMna8FkmaCuD9BFM2/hewElhN0AzUn5q1yafNXFV/R9B4Vd/qnABSWUlX8eNLx5k6Z4qZ82eYOmdqVgho2pHCzSqZVrrC2hkBu2YNDERMLvvQQ8Hr01zdl7HjtVCi2oWSFmBRO6/rdOnJfABxbfKttOs3o0ZLSakXOf/j+hKGx9/U0SD2NN1YExPu++0X/XVoZfoMzX6WjA7nAzgYOALYv67iuCmrSilOz+YDiJoDeNWq7gwqK/JoESmcrAZ71e9/xfoVs5qBBgcGWfihX7HlFwfO2T7q497ueMukMZTDw7BjR2/nC+hXncwH8DqCFNDXAheEP1d3u4CFEtVx3K17TTVaSoHENRVt/eXckz9EN8u02xGb9PzWrYqPyFzUbUH9AmwiuPK/LXz8ZOCqpNdlseQ+JWQ37jUV9ikp5TXto3trLZXttmrGva7oraJlbHKigzDQR9z9EQimhnT3H7Bveshq6cZM2wr7lJTyjNBp5Ya33ZvjZnMoFbUjt99iONJUAPeZ2UEEs4BdZ2afB5q03EmiblQkUgrt5uOB7k772Go5WrlOafeapnEOpU7mFuiVuNDU5cvLWQm0lA3UzJ4LDAFfdvedmZUqRqEnhRdpENfBmnbilm5N/N5pObISFWsRddJPu10vxGZKpdgd1J10Ao/UFoLJYG4DHptBGUX6SqdNON2a9jHPpqQ4aZtSJifhjDNmb3fGGfldbTeL1ch6QvsspGkC+mK43AlcHf5+dZaFKr3SzQsnWei0Cadb0z52sympW9KM8gU4+2zY2dDWsHNnsD4PzfotoHwpKBYkbeDuSwHM7Lvu/vTsi1RyjRO81C5toJj3hpKZkaGRyCacuBG+UcaXjnfcTNONcnRb2tDRLVuit4tbn7XaV3j5ctizZ+7zZYvmbmU+gPSdBQnM7OjKtGYAAA2dSURBVFVm9n0zmzGzOe1SpTU5GXwy0qaMkL7WrSacfilHvTIPhxkfD1JO90MKijR9AK8ws1cAB9V+Dx934nbgFQQDzPpD7co/6rIAyndvKB3rVhNOnuXotDUz7vXNQkfrXxM31cbwcGvl6La+ieaOGhxQvwAfi1guT3pdmgW4AViWdvvcB4I1kzSqZf78cowYEQl1OmYx6fVRA6qiXtO4DAzoq9QqYgaC9Xw076yDp6gAgBXABmDDyMhIRn+eLkgzIYxG/EqJtDLCN+pk3s4I4aTrqOFhfYXaEVcBJI4DMLPnAS8Nr/w/CAwD73L36xJe91Wiw0VXufvnw21uAN7u7qmC+ws9DiAps1VNq8njRHKSNm9hY9wDBM05jV1hca9Pc8wafX3a0/Y4AOBS4JfA14F/Bs4CPpD0Ind/gbs/NWL5fKuFL5RaA6UZLFgQ/IybiTqK+gKkJNJ21MaFdNZG9qbdb9JzoK9Pt6WpAHa6+/uBze5+vbv/N7A743IVU/3oFdjX4Rs1E3Xcp3/Rot6UVaRDaXP8xJ2U9+xpPVImKc6+DFFCZZKmAlhsZm8FhszsrWb2NoKpIdtmZi83s/uAZwJfNLNrO9lfz0Rd6tRs3w7XXLMvx8+6ddHTHf32txoYJqWQNtIl7qRc276VSJnaMaOifMoYZll0afoAzo9a7+4XZFKiJnLvA0hqoGxs3Fy8OHrEihoypY/E9QF0GhZZpBxAZRfXB9BSMri85V4BJHX0Np7YNfuXVIRO1sUWVwEkpoIwsyXAucBTmD0l5LFdLWEZrFkz91KnJur+dGQkusJQQ6b0mfFxnfDLKE0fwCTwA+Awgikhp4CbMyxTcbWawLxb00iKiGQgTR/ALe5+tJltdPcjw3U3u/sxPSlhndybgNqhe2MRyVnbTUDArvDnL8zsJcDPAcUypqV7YxEpqDQVwN+b2RDwNuAS4DHAWzItlYiIZC6xD8Ddr3b3be5+u7s/392Pdvcv9KJwlaTJZESkR9JEAUWe7N39xO4Xp+I0mYyI9FCaJqA/AF6XdUGE5vPkqQIQkS5LEwb6W3e/sXHJvGRl1m4zTtp58kREuiBNBXCUmT1gZr80s1vN7BIzW5x5ycqqPmGc+75mnMnJ5IqhzPPkiUjppOkEnk8Q9vlE4NUEqaHXZVyu8oprxjn77PiKoUYDx0Skh1JNCu/uM+7+sLv/2N3XAF/OuFzlFddcs2VL8mTxfTPRqIiUQexIYDNb4e5re1yepkoxEjjtzGA1SgwnIhlrZ0awMzMsT/+Ka8aJSnAOat8Xkdw0qwCsZ6XoJ3HNOBdfrPZ9ESmUZuMAyjNRQNE0y/+jxHAiUhDNKoCjzOzBiPUGuLs/JqMy9S8lhhORAomtAMLwTxER6VOpwkBFRKT/qAIQEakoVQBKvywiFZUmG2j/UvplEamwat8BNEu/3AndVYhICVT7DiCL9Mu6qxCRkqj2HUAW6ZezuqsQEemyalcAWaRf1qQuIrlR62trql0BZJF+WZO6iOSi2VxMEi02HXSmBzV7H3ACsBP4CXC6uz+Q9LpSpINu7AOA4K5Cef1FMhWXiX10FKamel2aYmknHXSWrgOe6u5HAj8C3pVTObpPk7qI5EKtr63LJQrI3b9S9/DbwCvzKEdmlPRNpOdGRqLvANT6Gq8IfQBnAF/KuxAiUm6aUrt1mVUAZvZVM7s9YjmpbptVwG4gtpvGzFaY2QYz27B58+asiisiJafW19bl0gkMYGanAW8AjnP37QmbAyXpBBYRKZhCdQKb2YuBc4ET0578c6XgYhHpQ3mlgvgQ8CjgOjMD+La7F3MSeqV2EJE+lVsTUDtyaQJScLGIlFyhmoBKRcHFItKnVAEkUWoHEelTqgCSKLhYRPqUKoAkCi4WkT5V7Qlh0lJqBxHpQ7oDEBGpKFUAIiIVpQpARKSiVAGIiFSUKgARkYpSBSAiUlGqAEREKkoVgIhUhjK7z6aBYCJSCcrsPpfuAESkElat2nfyr9m+PVhfVaoARKTvRDX1KLP7XGoCEpG+EtfUs2gRbNkyd/sqZ3bXHUC3qHdJpBDimnpAmd0bqQLohtolx/Q0uO+75FAlINJzcU06W7cqs3sjzQncDZo3WKQw9HWcS3MCZ0m9SyKFoUn80lMF0A2aN1ikMDSJX3qqALpBlxwihTI+HjT3zMwEP3Xyj6YKoBt0ySEiJaRxAN2ieYNFpGR0ByAiUlGqAEREKkoVgIhIRakCEBGpKFUAIiIVVapUEGa2Gagf5L0YuD+n4nST3kex6H0Ui95H50bdfUnjylJVAI3MbENUfouy0fsoFr2PYtH7yI6agEREKkoVgIhIRZW9AlibdwG6RO+jWPQ+ikXvIyOl7gMQEZH2lf0OQERE2qQKQESkokpfAZjZe8xso5ndZmZfMbPH512mdpjZ+8zsB+F7+ayZHZR3mdphZq8ys++b2YyZFSrkLYmZvdjMfmhmd5nZ3+RdnnaZ2eVm9mszuz3vsnTCzA41s6+b2R3hZ+rsvMvUDjPb38z+28y+F76PC/IuU03p+wDM7DHu/mD4+5uBP3T3M3MuVsvM7M+Ar7n7bjP7JwB3f2fOxWqZmf0BMAN8GHi7uxdwEue5zGw+8CPghcB9wM3Aye5+R64Fa4OZPQd4CPg3d39q3uVpl5k9Dnicu99qZo8GbgFeVrb/iZkZcIC7P2RmA8A3gbPd/ds5F638dwC1k3/oAKCUNZq7f8Xdd4cPvw08Ic/ytMvd73T3H+ZdjjY8A7jL3X/q7juBK4GTci5TW9z9JmBr3uXolLv/wt1vDX//LXAncEi+pWqdBx4KHw6ESyHOU6WvAADMbI2Z3QuMA+/OuzxdcAbwpbwLUTGHAPfWPb6PEp5s+pWZjQFPB76Tb0naY2bzzew24NfAde5eiPdRigrAzL5qZrdHLCcBuPsqdz8UmATemG9p4yW9j3CbVcBugvdSSGneh0i3mNmBwGeAcxru+EvD3fe4+9MI7uyfYWaFaJorxZSQ7v6ClJtOAtcA52dYnLYlvQ8zOw14KXCcF7hzpoX/R5n8DDi07vETwnWSo7DN/DPApLtflXd5OuXuD5jZ14EXA7l30pfiDqAZMzui7uFJwA/yKksnzOzFwLnAie6+Pe/yVNDNwBFmdpiZ7Qe8BvhCzmWqtLDz9F+BO939g3mXp11mtqQW1WdmCwkCDQpxnuqHKKDPAE8iiDyZBs5099JduZnZXcCjgC3hqm+XNJrp5cAlwBLgAeA2d39RvqVKx8yOBy4C5gOXu/uanIvUFjO7AngeQfrhXwHnu/u/5lqoNpjZnwLfADYRfL8BznP3a/IrVevM7EhgHcHnah7w7+7+d/mWKlD6CkBERNpT+iYgERFpjyoAEZGKUgUgIlJRqgBERCpKFYCISEWpApBMmdmeMFNrbSldaGuVmdlzzOxWM9ttZq/MuzzSXaUYCSyltiMcAi/ldA9wGvD2nMshGdAdgOSm7u7gLjO7Olx3gpl9x8y+G+Yc+r1w/Woz+1k4X8IPzOzYcP3H669Mw5xEY+HvnzOzW8Ic7CvqtvmrcB+3mdk2M3teRNmmzGyxmR1oZv8ZpuvGzI4Ly7YpzLv/qLrtr6x7/ZVmNhX+fpqZba67C9ocpv1I2t/i8PfFdfuab8HcETeHf4s3hOufV/sbho/fHv7Nnh0e8w4z21ErQ7jNu8P93G5ma8ORt7O4+5S7b2TfQCzpI6oAJBcW5N9/OLw7eF3dU98E/sTdn06QkvncuucudPcjCdIDvDTFYc5w96OBZcCbzWw4XP9e4Dnhsb/R5PUDwKeBS939K2a2P/Bx4NXuvpTgDnpl3faPM7ODzWwR8LiGfX3K3Z8WHvNT4d8gaX9R/grY5u7HAMcArzezw+I2dvdvhMc8HvhJXRkAPuTux4RzBiwk3d9U+ogqAMnLQuCRiPVPAK41s03AO4Cn1D33FjO7A3gn8LG69e+ru7J9Yt36N5vZ9wjmVzgUqOWNmgEenaKMHyGYkKSWmfVJwN3u/qPw8TrgOXXbXwH8n3D5ZIr9J+3v6+F7+nrduj8DXhuu/w4wXPe+nl33d3hLiuM/P7zb2gQcy+y/tVSAKgDJy+OBn0esv4TgynQp8AZg/7rnLnT3PyRI1PaBuvXvqLuy/QkETSLAC4BnuvtRwHfr9rUS+C8Lpkx8dpMy/hj4npmdkfI9fQE4MVzWp3xNM88P39Pz69YZ8Kba+3X3w9z9K+Fz36j7O1zYbMfh3celwCvDv/VHmP23lgpQBSB5+UvgPyPWD7EvDfPymNc+SJDorJkh4Dfuvt3Mngz8Sd1zPwe+BxxF8yagNcBbgXPDvogfAmNmdnj4/KnAjXXb7yS42/hW+HuSpP1FuRZYaUGaZMzsf5nZASmO1ah2sr/fgnz7ivCpIEUBSc9ZMHfzs4g+wa8GPm1mvwG+BtS3b7/FzE4h+NwmRaV8GTjTzO4kONF+Ozz2MPD/CNJu74no95zF3beY2d8Bl7j7X5rZ6WH5FhCkkL6sYfvzw+MkVVC4+yNJ+4vwUWAMuDXstN0MvCzpWBHHfsDMPkKQk/6X4bHnMLNjgM8CBwMnmNkF7q6moj6hbKAiIhWlJiARkYpSBSAiUlGqAEREKkoVgIhIRakCEBGpKFUAIiIVpQpARKSi/gdg+KXa7MCj8AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "y = iris.target\n",
    "for c, i in zip(\"rgb\", [0, 1, 2]):\n",
    "    plt.scatter(Z[y==i, 0], Z[y==i, 1], c=c)\n",
    "plt.xlabel('Главная компонента 1')\n",
    "plt.ylabel('Главная компонента 2')\n",
    "plt.title('PCA датасета IRIS')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y7P4Be8PdfKQ"
   },
   "source": [
    "Таким образом, мы перешли от четырехмерного пространства признаков к двумерному и при этом классы остались разделимы в пространстве, то есть классификация возможна."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q896KWx0dfKR"
   },
   "source": [
    "PCA наиболее хорошо работает, когда собственные значения $\\lambda$ на каком-то участке графика распределения убывают скачкообразно (критерий крутого склона), другими словами, если существуют предпосылки к тому, что следует решать задачу в пространстве меньшей размерности. Если же они убывают монотонно, следует рассмотреть вариант использования других методов работы с пространством признаков."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y6YHvcPvdfKS"
   },
   "source": [
    "## Литература"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pQKbma5FdfKT"
   },
   "source": [
    "1. [Методы отбора признаков](https://habr.com/ru/company/aligntechnology/blog/303750/)\n",
    "2. [Взаимная информация](https://ru.wikipedia.org/wiki/%D0%92%D0%B7%D0%B0%D0%B8%D0%BC%D0%BD%D0%B0%D1%8F_%D0%B8%D0%BD%D1%84%D0%BE%D1%80%D0%BC%D0%B0%D1%86%D0%B8%D1%8F)\n",
    "3. [Методы понижения размерности](http://www.machinelearning.ru/wiki/images/0/06/SLT%2C_lecture_8.pdf)\n",
    "4. [Лемма о малом искажении](https://ru.wikipedia.org/wiki/%D0%9B%D0%B5%D0%BC%D0%BC%D0%B0_%D0%BE_%D0%BC%D0%B0%D0%BB%D0%BE%D0%BC_%D0%B8%D1%81%D0%BA%D0%B0%D0%B6%D0%B5%D0%BD%D0%B8%D0%B8)\n",
    "5. [PCA from Scratch in Python](https://github.com/bhattbhavesh91/pca-from-scratch-iris-dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Снижать размерность данных можно за счет\n",
    "    * отбора признаков (корреляция, взаимная информация, вес коэфф. регрессии, feature_importances_ и др.)\n",
    "    * снижения размерности (случайные проекции, PCA, ICA (Independent Component Analysis), NMF (Non-negative Matrix Factorization) и др.)\n",
    "* Уменьшение размерности \n",
    "    * ускоряет работу моделей\n",
    "    * улучшает интерпретируемость решения\n",
    "    * упрощает поддержку решения\n",
    "    * улучшает точность модели, если были удалены шумовые и нерелевантные признаки\n",
    "* В основе PCA используется понятие [_собственного вектора_](https://ru.wikipedia.org/wiki/%D0%A1%D0%BE%D0%B1%D1%81%D1%82%D0%B2%D0%B5%D0%BD%D0%BD%D1%8B%D0%B9_%D0%B2%D0%B5%D0%BA%D1%82%D0%BE%D1%80) - это вектор, умножение которого на матрицу даёт коллинеарный вектор - тот же вектор, умноженный на некоторое число, называемое _собственным значением_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q&A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__1. Можно ли отобрать наиболее значимые признаки с помощью PCA?__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Да, но следует помнить, что PCA не выбирает из имеющихся признаков наиболее значимые, а строит новые, которые \"учтут максимум информации\" из имеющихся признаков, создав тем самым новое уменьшенное признаковое пространство."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__2. Какие применения на практике есть у матричных разложений, например, SVD?__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сингулярное разложение широко применяется, в частности, при сжатии данных, обработке сигналов, численных итерационных методах для работы с матрицами, латентно-семантическом анализе, а также в _рекомендательных системах_. Так, имея разреженную матрицу \"Users\"-\"Movies\" с оценками фильмов пользователями, мы хотим рекомендовать пользователям новые фильмы, предсказывая оценку i-го пользователя j-му фильму. Для этого мы можем сделать SVD разложение этой матрицы и затем вектор, описывающий i-го пользователя, умножить на вектор, описывающий j-ый фильм. Так мы предскажем оценку, которую данный пользователь поставит этому фильму. Значит, можем сделать это для всех пользователей/фильмов? :)\n",
    "\n",
    "Материалы на тему: [ссылка 1](https://habr.com/ru/company/yandex/blog/241455/), [ссылка 2](https://habr.com/ru/company/surfingbird/blog/139863/), [ссылка 3](https://habr.com/ru/company/okko/blog/454224/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__3. Как вручную найти собственные вектора и собственные значения?__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим эту задачу на примере следующей матрицы:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$A = \\begin{Vmatrix}4 & -3 & 3\\\\1 & 2 & 1\\\\1 & 1 & 2\\end{Vmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Составим характеристическое уравнение:\n",
    "$$det(A-\\lambda E) = \\begin{Vmatrix}4-\\lambda & -3 & 3\\\\1 & 2-\\lambda & 1\\\\1 & 1 & 2-\\lambda\\end{Vmatrix} = 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Расписываем детерминант (определить) матрицы:\n",
    "\n",
    "$$(4-\\lambda)(2-\\lambda)^2-3-3-(4-\\lambda)+3(2-\\lambda)+3(2-\\lambda) = 0$$\n",
    "$$(\\lambda-1)(\\lambda^2-7\\lambda-18) = 0$$\n",
    "\n",
    "Отсюда получаем совокупность уравнений:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\left[ \n",
    "    \\begin{gathered}\n",
    "        \\lambda-1 = 0 \\Rightarrow \\lambda = 1\n",
    "        \\\\\n",
    "        \\lambda^2-7\\lambda-18 = 0 \\Rightarrow D = (-7)^2-4*18=-23 < 0\n",
    "    \\end{gathered}\n",
    "\\right.$$\n",
    "\n",
    "Получили, что характеристическое уравнение имеет только один действительный корень $\\lambda=1$, а матрица $A$ только одно собственное значение."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Найдём собственный вектор для полученного собственного значения, решая уравнение:\n",
    "\n",
    "$$\\begin{Vmatrix}4-\\lambda & -3 & 3\\\\1 & 2-\\lambda & 1\\\\1 & 1 & 2-\\lambda\\end{Vmatrix} * \\begin{Vmatrix}x_1\\\\x_2\\\\x_3\\end{Vmatrix} = \\begin{Vmatrix}0\\\\0\\\\0\\end{Vmatrix}$$\n",
    "\n",
    "Его можно решать в матричном виде, а можно, подставив найденное значение $\\lambda = 1$, переписать в виде системы уравнений:\n",
    "\n",
    "$$\n",
    "\\begin{cases}\n",
    "    3 x_1 - 3 x_2 - 3x_3 = 0,\n",
    "    \\\\\n",
    "    x_1 + x_2 + x_3 = 0,\n",
    "    \\\\\n",
    "    x_1 + x_2 + x_3 = 0.\n",
    "    \\end{cases}\n",
    "$$\n",
    "\n",
    "Отсюда получаем решение системы:\n",
    "\n",
    "$$\n",
    "\\begin{cases}\n",
    "    x_1 = 0,\n",
    "    \\\\\n",
    "    x_3 = -x_2.\n",
    "    \\end{cases}\n",
    "$$\n",
    "\n",
    "Собственный вектор равен:\n",
    "\n",
    "$$\\overrightarrow{x} = \\begin{Vmatrix}0\\\\x_2\\\\-x_2\\end{Vmatrix} = C*\\begin{Vmatrix}0\\\\1\\\\-1\\end{Vmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__4. Какими свойствами обладают собственные вектора?__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пусть $A: X_n → X_n$ — линейный оператор.\n",
    "\n",
    "1. Все собственные векторы линейного оператора, соответствующие одному и тому же собственному значению, вместе с нулевым вектором образуют линейное пространство.\n",
    "2. Собственные векторы линейного оператора, соответствующие различным собственным значениям, линейно независимы.\n",
    "3. Если линейный оператор $A: X_n → X_n$ имеет n различных (вещественных) собственных значений, то собственные векторы, соответствующие этим собственным значениям, образуют базис в $X_n$. Такой базис называется собственным базисом линейного оператора $A$.\n",
    "4. Матрица $A$ линейного оператора $A: X_n → X_n$ в некотором базисе $x_1, x_2, ..., x_n$ имеет диагональный вид тогда и только тогда, когда этот базис собственный, причем диагональные элементы этой матрицы — собственные значения оператора $λ_1, λ_2, ..., λ_n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__5. С отбором признаков понятно, но что с предобработкой данных, напомните, есть какой-то \"джентльменский набор\"?__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Методы предварительной обработки данных:\n",
    "\n",
    "- __очистка данных__ - является процессом обнаружения и исправления или удаления ошибочных записей данных\n",
    "- __нормализация данных__ - используется для стандартизации диапазона значений независимых переменных или признаков данных (сведение к интервалам [0, 1] или [-1, +1])\n",
    "- __преобразование данных__ - является процессом приведения данных в формат, который ожидают люди\n",
    "- __выделение признаков__ - является процессом преобразования входных данных в набор признаков, которые могут хорошо представлять входные данные\n",
    "- __уплотнение данных__ - является преобразованием числовых данных в исправленный, упорядоченный и упрощённый вид, что приводит к уменьшению количества или размерности данных\n",
    "\n",
    "Методы обнаружения выбросов:\n",
    "\n",
    "- __статистические подходы__ (box-plot, гистограмма и т.д.)\n",
    "- __модельные тесты__ (строим модель, описывающую данные, те объекты, что сильно выбиваются - выбросы)\n",
    "- __метрические методы__ (мерой аномальности может служить, например «расстояние до k-го соседа»)\n",
    "- __методы машинного обучения__\n",
    "    - __AdaBoost__ - выбросы набирают вес при построении новых моделей, решение - исключаем объекты из выборки, которые быстро набирают вес\n",
    "    - __метод опорных векторов для одного класса (OneClassSVM)__ - скорее алгоритм поиска новизны, а не выбросов, т.к. «затачивается» под обучающую выборку\n",
    "    - __изолирующий лес (IsolationForest)__ - одна из вариаций случайного леса, каждое дерево строится до исчерпании выборки, при разбиении выбирается случайные признак и расщепление, для каждого объекта мера его нормальности – среднее арифметическое глубин листьев, в которые он попал (изолировался)\n",
    "    - __эллипсоидальная аппроксимация данных (EllipticEnvelope)__ - облако точек моделируется как внутренность эллипсоида, метод хорошо работает только на одномодальных данных, а совсем хорошо – на нормально распределённых    \n",
    "\n",
    "Восстановление данных - заполнение пропусков:\n",
    "\n",
    "- __\"средние\"__: мат. ожидание, медиана, мода\n",
    "- __предсказание моделью__ (например, линейная регрессия)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__6. Какие способы подбора гиперпараметров существуют, помимо простого перебора?__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- \"Эль Классико\"\n",
    "    - __GridSearchCV__ - поиск по решетке, все комбинации\n",
    "    - __RandomizedSearchCV__ - случайный поиск\n",
    "- Продвинутые методы\n",
    "    - __байесовская оптимизация__ - стохастическая модель функции отображения из значений гиперпараметра в целевую функцию, ищется оптимальный набор гиперпараметров\n",
    "    - __оптимизация на основе градиентов__ - вычисление градиента гиперпараметров и оптимизация их с помощью градиентного спуска\n",
    "    - __эволюционные алгоритмы__ - начинаем со случайных, оцениваем пригодность, \"скрещиваем\" и \"мутируем\" слабые, повторяем до сходимости\n",
    "\n",
    "Популярные библиотеки:\n",
    "\n",
    "- __HyperOpt__ - автоматическая оптимизация гиперпараметров - https://github.com/hyperopt/hyperopt\n",
    "- __AutoML__ - автоматизирование workflow ML задач - http://docs.h2o.ai/h2o/latest-stable/h2o-docs/automl.html\n",
    "- __Spearmint__ и __BayesOpt__ - байесовская оптимизация - https://github.com/HIPS/Spearmint, https://rmcantin.bitbucket.io/html/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7. Как будет работать случайный лес с категориальными признаками?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Категориальные признаки нельза преобразовать в dummy переменные, потому что они будут разреженными (там будет много нулей). А мы уже говорили, что случайному лесу тяжело находить зависимость целевой переменной от разреженного признака, плюс теряется \"мощь\" признака, так как один будет размазываться по нескольким\n",
    "https://roamanalytics.com/2016/10/28/are-categorical-variables-getting-lost-in-your-random-forests/\n",
    "\n",
    "В случае с категориальными переменными можно построить n-арное дерево\n",
    "\n",
    "Также в ансамблях (типа Catboost, LGBM) есть возможность явно указать категориальные признаки и они будут правильно учтены в деревьях без дамми (one hot encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8. Метод локтя и Дендрограммы** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Метод Локтя\n",
    "<img src=\"data/elbow.png\" style=\"width: 600px;\">\n",
    "2. Дендрограммы\n",
    "<img src=\"data/dendro.png\" style=\"width: 600px;\">\n",
    "https://wiki.loginom.ru/articles/dendrogram.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Lesson_8.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
